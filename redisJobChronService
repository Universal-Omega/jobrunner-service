#!/usr/bin/env php
<?php

if ( PHP_SAPI !== 'cli' ) {
	die( "This is not a valid entry point.\n" );
}

require( __DIR__ . '/src/RedisJobService.php' );

RedisJobService::checkEnvironment();

class RedisJobChronService extends RedisJobService {
	const TASK_PERIOD_SEC = 60;

	/**
	 * Entry point method that starts the service in earnest and keeps running
	 */
	public function main() {
		$this->notice( "Starting job chron loop(s)..." );

		$host = gethostname();

		// Setup signal handlers...
		$handlerFunc = function( $signo ) {
			print "Caught signal ($signo)\n";
			exit( 128 + $signo );
		};
		$ok = pcntl_signal( SIGHUP, $handlerFunc )
			&& pcntl_signal( SIGINT, $handlerFunc )
			&& pcntl_signal( SIGTERM, $handlerFunc );
		if ( !$ok ) {
			throw new Exception( 'Could not install singal handlers.' );
		}


		// This is randomized to scale the liveliness with the # of runners.
		$lastPeriodicTime =  time() - mt_rand( 0, self::TASK_PERIOD_SEC );

		$memLast = memory_get_usage();
		$this->incrStats( "start-chron.$host", 1 );
		while ( true ) {
			pcntl_signal_dispatch();

			// Do tasks on a simple interval cycle...
			if ( ( time() - $lastPeriodicTime ) > self::TASK_PERIOD_SEC ) {
				$lastPeriodicTime = time();

				$count = $this->syncAggregatorRegistry();
				$this->notice( "Synced $count aggregator(s) to the current one." );
				$this->incrStats( "periodictasks.sync.$host", 1 );

				$count = $this->executePeriodicTasks();
				$this->notice( "Updated the state of $count job(s) (recycle/undelay/abandon)." );
				$this->incrStats( "periodictasks.done.$host", 1 );
			}

			sleep( 1 );

			// Track memory usage
			$memCurrent = memory_get_usage();
			$this->debug( "Memory usage: $memCurrent bytes." );
			$this->incrStats( "memory.$host", $memCurrent - $memLast );
			$memLast = $memCurrent;
		}
	}

	/**
	 * Merge wiki and queue type registration data into all aggregators
	 *
	 * This makes sure that the data is not lost on aggregator failure
	 *
	 * @return integer Servers fully updated
	 */
	private function syncAggregatorRegistry() {
		$count = count( $this->aggrSrvs );

		try {
			$types = $this->redisCmdHA(
				$this->aggrSrvs,
				'hGetAll',
				array( $this->getQueueTypesKey() )
			);
			if ( $types ) {
				$okCount = $this->redisCmdBroadcast(
					$this->aggrSrvs,
					'hMSet',
					array( $this->getQueueTypesKey(), $types )
				);
				$count = min( $count, $okCount );
			}

			$wikis = $this->redisCmdHA(
				$this->aggrSrvs,
				'sMembers',
				array( $this->getWikiSetKey() )
			);
			if ( $wikis ) {
				$okCount = $this->redisCmdBroadcast(
					$this->aggrSrvs,
					'sAdd',
					array_merge( array( $this->getWikiSetKey() ), $wikis )
				);
				$count = min( $count, $okCount );
			}
		} catch ( RedisExceptionHA $e ) {
			$count = 0;
		}

		return $count;
	}

	/**
	 * Recycle or destroy any jobs that have been claimed for too long
	 * and release any ready delayed jobs into the queue. Also abandon
	 * and prune out jobs that failed too many times. This updates the
	 * aggregator server as necessary.
	 *
	 * @note: similar to JobQueueRedis.php periodic tasks method
	 * @return int|bool Number of jobs recycled/deleted/undelayed/abandoned (false if not run)
	 */
	private function executePeriodicTasks() {
		$jobs = 0;

		$host = gethostname();

		$ok = true;
		try {
			$types = $this->redisCmdHA(
				$this->aggrSrvs,
				'hKeys',
				array( $this->getQueueTypesKey() )
			);
			$wikiIds = $this->redisCmdHA(
				$this->aggrSrvs,
				'sMembers',
				array( $this->getWikiSetKey() )
			);
			if ( !is_array( $types ) || !is_array( $wikiIds ) ) {
				$this->incrStats( "periodictasks.failed.$host", 1 );
				return $jobs;
			}

			// Randomize to scale the liveliness with the # of runners
			shuffle( $types );
			shuffle( $wikiIds );

			$now = time();

			// Build up the script arguments for each queue...
			$paramsByQueue = array();
			foreach ( $types as $type ) {
				$ttl = $this->getTTLForType( $type );
				$attempts = $this->getAttemptsForType( $type );
				foreach ( $wikiIds as $wikiId ) {
					$paramsByQueue[] = array(
						'queue'  => array( $type, $wikiId ),
						'params' => array(
							"{$wikiId}:jobqueue:{$type}:z-claimed", # KEYS[1]
							"{$wikiId}:jobqueue:{$type}:h-attempts", # KEYS[2]
							"{$wikiId}:jobqueue:{$type}:l-unclaimed", # KEYS[3]
							"{$wikiId}:jobqueue:{$type}:h-data", # KEYS[4]
							"{$wikiId}:jobqueue:{$type}:z-abandoned", # KEYS[5]
							"{$wikiId}:jobqueue:{$type}:z-delayed", # KEYS[6]
							$now - $ttl, # ARGV[1]
							$now - 7 * 86400, # ARGV[2]
							$attempts, # ARGV[3]
							$now # ARGV[4]
						),
						'keys'   => 6 # number of first argument(s) that are keys
					);
				}
			}
			// Batch the Lua queries to avoid client OOMs
			$paramsByQueueBatches = array_chunk( $paramsByQueue, 1000 );

			$mapSet = array(); // ready map of (queue name => timestamp)
			// Run the script on all job queue partitions...
			foreach ( $this->queueSrvs as $qServer ) {
				foreach ( $paramsByQueueBatches as $paramsByQueueBatch ) {
					$ok = $this->updateQueueServer(
						$qServer, $paramsByQueueBatch, $mapSet, $jobs ) && $ok;
				}
			}
			// Update the map in the aggregator as queues become ready
			$this->redisCmdHA(
				$this->aggrSrvs,
				'hMSet',
				array( $this->getReadyQueueKey(), $mapSet )
			);
		} catch ( RedisExceptionHA $e ) {
			$ok = false;
		}

		if ( !$ok ) {
			$this->error( "Failed to do periodic tasks for some queues." );
		}

		return $jobs;
	}

	private function updateQueueServer( $qServer, array $paramsByQueue, array &$mapSet, &$jobs ) {
		$host = gethostname();

		$qConn = $this->getRedisConn( $qServer );
		if ( !$qConn ) {
			$this->incrStats( "periodictasks.failed.$host", count( $paramsByQueue ) );
			return false; // partition down
		}

		static $script =
<<<LUA
		local kClaimed, kAttempts, kUnclaimed, kData, kAbandoned, kDelayed = unpack(KEYS)
		local rClaimCutoff, rPruneCutoff, rAttempts, rTime = unpack(ARGV)
		local released,abandoned,pruned,undelayed,ready = 0,0,0,0,0
		-- Get all non-dead jobs that have an expired claim on them.
		-- The score for each item is the last claim timestamp (UNIX).
		local staleClaims = redis.call('zRangeByScore',kClaimed,0,rClaimCutoff)
		for k,id in ipairs(staleClaims) do
			local timestamp = redis.call('zScore',kClaimed,id)
			local attempts = redis.call('hGet',kAttempts,id)
			if attempts < rAttempts then
				-- Claim expired and attempts left: re-enqueue the job
				redis.call('lPush',kUnclaimed,id)
				released = released + 1
			else
				-- Claim expired and no attempts left: mark the job as dead
				redis.call('zAdd',kAbandoned,timestamp,id)
				abandoned = abandoned + 1
			end
			redis.call('zRem',kClaimed,id)
		end
		-- Get all of the dead jobs that have been marked as dead for too long.
		-- The score for each item is the last claim timestamp (UNIX).
		local deadClaims = redis.call('zRangeByScore',kAbandoned,0,rPruneCutoff)
		for k,id in ipairs(deadClaims) do
			-- Stale and out of attempts: remove any traces of the job
			redis.call('zRem',kAbandoned,id)
			redis.call('hDel',kAttempts,id)
			redis.call('hDel',kData,id)
			pruned = pruned + 1
		end
		-- Get the list of ready delayed jobs, sorted by readiness (UNIX timestamp)
		local ids = redis.call('zRangeByScore',kDelayed,0,rTime)
		-- Migrate the jobs from the "delayed" set to the "unclaimed" list
		for k,id in ipairs(ids) do
			redis.call('lPush',kUnclaimed,id)
			redis.call('zRem',kDelayed,id)
		end
		undelayed = #ids
		ready = redis.call('lLen',kUnclaimed)
		return {released,abandoned,pruned,undelayed,ready}
LUA;

		$failed = 0;
		try {
			$sha1 = $this->redisCmd( $qConn, 'script', array( 'load', $script ) );

			$this->redisCmd( $qConn, 'multi', array( Redis::PIPELINE ) );
			foreach ( $paramsByQueue as $params ) {
				$this->redisCmd( $qConn, 'evalSha',
					array( $sha1, $params['params'], $params['keys'] ) );
			}
			$res = $this->redisCmd( $qConn, 'exec' );

			$now = time();
			foreach ( $res as $i => $result ) {
				if ( !$result ) {
					++$failed;
					continue;
				}
				list( $qType, $qWiki ) = $paramsByQueue[$i]['queue'];
				list( $released, $abandoned, $pruned, $undelayed, $ready ) = $result;
				if ( $released > 0 || $undelayed > 0 || $ready > 0 ) {
					// This checks $ready to handle lost aggregator updates as well as
					// to merge after network partitions that caused aggregator fail-over.
					$mapSet[$this->encQueueName( $qType, $qWiki )] = $now;
				}
				$jobs += ( array_sum( $result ) - $ready );
				$this->incrStats( "job-recycle.$qType.$qWiki", $released );
				$this->incrStats( "job-abandon.$qType.$qWiki", $abandoned );
				$this->incrStats( "job-undelay.$qType.$qWiki", $undelayed );
			}
		} catch ( RedisException $e ) {
			$failed += count( $paramsByQueue );
			$this->handleRedisError( $e, $qServer );
		}

		$this->incrStats( "periodictasks.failed.$host", $failed );

		return ( $failed == 0 );
	}

	/**
	 * @param string $type Queue type
	 * @return integer Seconds
	 */
	protected function getTTLForType( $type ) {
		return isset( $this->claimTTLMap[$type] )
			? $this->claimTTLMap[$type]
			: $this->claimTTLMap['*'];
	}

	/**
	 * @param string $type Queue type
	 * @return integer
	 */
	protected function getAttemptsForType( $type ) {
		return isset( $this->attemptsMap[$type] )
			? $this->attemptsMap[$type]
			: $this->attemptsMap['*'];
	}
}

error_reporting( E_ALL | E_STRICT );
ini_set( 'display_errors', 1 );

// Run the server...
set_time_limit( 0 );
ini_set( 'memory_limit', '256M' );
RedisJobChronService::init(
	getopt( '', array( 'config-file::', 'help', 'verbose' ) )
)->main();
